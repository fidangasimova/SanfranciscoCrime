---
title: "PROJECT II: San Francisco Crime (2016)"
author: "FH"
date: "December 28, 2020"
---output: html_document
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_meta(clean=T)
```



# Inroduction

The analysis performed within this project is based on a data set entitled San Francisco Crime is available on Kaggle and can be found here:


- https://www.kaggle.com/roshansharma/sanfranciso-crime-dataset 

The original dataset is from https://datasf.org/opendata/, the central clearinghouse for data published by the City and County of San Francisco. The San Francisco Police Department does not guarantee the accuracy, completeness, timeliness or correct sequencing of the information as the data is subject to change as modifications and updates are completed. The complete data set can be found here:


- https://data.sfgov.org/Public-Safety/Police-Department-Incident-Reports-Historical-2003/tmnf-yvry

Due to computation power issues only a subset of the data was used. The algorithms were developed using the **sf_crime** data, which is a subset of the of the SF Opendata and includes only 2016. For the evaluation of the recommendation algorithm a **validation** data set was generated. The **validation** data set was only used in the final step to test the final algorithm and contained only 20% of **sf_crime** data.

The following libraries were used:  

```{r warning= FALSE, include = FALSE}
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")
if(!require(factoextra)) install.packages("factoextra", repos = "http://cran.us.r-project.org")
if(!require(Hmisc)) install.packages("Hmisc", repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
```


```{r  message=FALSE}
library(tinytex)
library(factoextra)
library(ggthemes)
library(Hmisc)
library(tidyr)
library(dslabs)
library(tidyverse)
library(caret)
library(data.table)
library(corrplot)
library(dplyr)
library(ggplot2)
library(lubridate)
library(stringr)
library(readr)
library(gridExtra)
```


```{r echo = FALSE}
#  Code to download data 
sf_crime <- read.csv("sf_crime.csv", header=TRUE, sep = ",", row.names = NULL, quote = "\"")

#  Names of columns without spaces
names(sf_crime) <- make.names(names(sf_crime), unique=TRUE)

#  Change the format from scientific to numerical
options(scipen = 999)

#  #  #  #  #  #  #  #  #  #  #  #  #  
#    Data Processing                #  
#  #  #  #  #  #  #  #  #  #  #  #  #  

sf_crime<-as.data.frame(sf_crime, stringsAsFactors=TRUE) %>%
         mutate(IncidntNum = as.numeric(IncidntNum),
                PdId       = as.numeric(PdId),
                Category   = factor(Category, levels = c("MISSING PERSON", "LARCENY/THEFT", "OTHER OFFENSES", "BURGLARY",                   
                                                "SUSPICIOUS OCC","WARRANTS", "ASSAULT","NON-CRIMINAL",               
                                                "STOLEN PROPERTY", "WEAPON LAWS", "DRUG/NARCOTIC","EMBEZZLEMENT",               
                                                "RUNAWAY", "DRUNKENNESS", "FORGERY/COUNTERFEITING","ROBBERY",                     
                                                "VEHICLE THEFT", "FRAUD", "SEX OFFENSES, FORCIBLE", "SECONDARY CODES",            
                                                "KIDNAPPING", "VANDALISM", "PROSTITUTION", "DRIVING UNDER THE INFLUENCE",
                                                "RECOVERED VEHICLE", "SEX OFFENSES, NON FORCIBLE","TRESPASS", "ARSON",                      
                                                "DISORDERLY CONDUCT", "LIQUOR LAWS", "FAMILY OFFENSES","EXTORTION",                  
                                                "BAD CHECKS", "LOITERING", "SUICIDE", "BRIBERY", "GAMBLING","PORNOGRAPHY/OBSCENE MAT", "TREA")),
              Descript    = as.character(Descript) %>% factor(),
              Resolution  = as.character(Resolution),
              DayOfWeek   = factor(DayOfWeek, levels = c("Monday", "Tuesday", "Wednesday",
                                                  "Thursday","Friday", "Saturday", "Sunday")),
               PdDistrict = as.character(PdDistrict))




```

## Data Exploration and Data Processing
The **sf_cime** data set contains 150,500 observations and 13 variables represented in 13 columns. Each row represents one incident with an ID given by the Police Department (*PdId*).

```{r }
#  Number of variables
print(ncol(sf_crime))
```

```{r }
#  Number of observations
print(nrow(sf_crime))
```

This data set contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from January to December 2016. The downloaded data set **sf_crime** consists of following 13 variables: 

- *PdId* - ID given by Police Department District
- *IncidntNum* - ID of th incident
- *Date* - time in mm/dd/yy format
- *Time* - time of the day from 00 to 24 hours
- *Category* - category of the crime incident
- *Descript* - detailed description of the crime incident
- *DayOfWeek* - the day of the week (Mo-Fr)
- *PdDistrict* - name of the Police Department District
- *Resolution* - how the crime incident was resolved 
- *Address* - the approximate street address of the crime incident
- *X* - Longitude
- *Y* - Latitude
- *Location* -  Longitude and Latitude combined in one variable

The data set **sf_crime** contains no missing data.
```{r }
# Check missing values
sum(is.na(sf_crime))
```

In order to visualize and analyze the data set several new variables were created through transformation of existing variables.

The variable *Date* was transformed and split into *Date_Month*, *Date_Day*, *Date_Year*. Variable *Time* was converted into a numerical variable *Time_Hour* that shows time of the day when crime took place. *Resolution_dummy* was created to indicate if the incident was resolved which equals 1 or not resolved = 0. In order to simplify variable *Category*  a dummy variable *Category_Violent_dummy* was assigned value 1 when "ASSAULT", "ROBBERY", "SEX OFFENSES, FORCIBLE", "KIDNAPPING" and value 0 otherwise. New variable *Count_Address_Occur* indicates how many times incidents took place at the same address. 

```{r echo = FALSE}
#   Convert char variable "Time_Hour" into a numeric variable
hhmm2dec <- function(x) {
  xlist <- strsplit(x,split=":")
  h <- as.numeric(sapply(xlist,"[",1))
  m <- as.numeric(sapply(xlist,"[",2))
  xdec <- h+(m/60)
  return(xdec)
}

#   Round variable "Time_Hour" without minutes
sf_crime$Time_Hour <- round(hhmm2dec(sf_crime$Time), 0)

#  Replace Hours = 24 with Hours = 0
sf_crime$Time_Hour <-replace(sf_crime$Time_Hour, sf_crime$Time_Hour ==24, 0) 

#   Convert "Date" character into class Date
sf_crime <- transform(sf_crime, Date_time = as.Date(Date, "%m/%d/%Y", tz = "UTC"))

#   Create variables "Date_month", "Date_day", "Date_year"
sf_crime<-sf_crime %>% mutate(Date_Month = month(Date_time), 
                              Date_Day   = day(Date_time),
                              Date_Year  = year(Date_time)) %>%
                       mutate(Month = month.name[Date_Month])
                      
#   Convert "Month" variable into an ordered factor                      
sf_crime<- sf_crime %>% mutate(Month = factor(Month, levels = month.name))

# Create new dummy variable "Resolution_dummy" (1 = resolved, 0 = unresolved)
sf_crime$Resolution_dummy <- ifelse(sf_crime$Resolution == "NONE", 0, 1)

# Create a new dummy variable "Category_violent" (1 = violent, 0 = nonviolent)
sf_crime$Category_Violent_dummy <- ifelse (sf_crime$Category %in% c("ASSAULT", "ROBBERY", "SEX OFFENSES, FORCIBLE", "KIDNAPPING"), 1, 0)  

# Create a new variable "Count_Address_Occur" to count the number of Occurrences of Addresses 
sf_crime<- sf_crime %>% group_by(Address) %>%
             mutate(Count_Address_Occur = n(), .groups = 'drop')  %>% ungroup() %>%
             arrange(desc(Count_Address_Occur))  

# Create a new variable "Count_Category_Occur" to count the number of Occurrences of Categories 
sf_crime<- sf_crime %>% group_by(Category) %>%
             mutate(Count_Category_Occur = n(), .groups = 'drop')  %>% ungroup() %>%
             arrange(desc(Count_Category_Occur))  



# Reorder columns of sf_crime data
sf_crime<-sf_crime[, c("PdId",  "IncidntNum", "Category", "Count_Category_Occur", "Category_Violent_dummy", "Descript",  "Resolution","Resolution_dummy", 
                      "DayOfWeek","Date_Day", "Date_Year", "Date_Month", "Month", "Time_Hour", "Date_time",                                                       
                      "PdDistrict", "Address", "Count_Address_Occur", "Y",  "X", "Location")]

# Order the data by increasing incident number "PdId"(police department ID)
sf_crime <- arrange(sf_crime, by = PdId)

```

```{r }
# Summary of the data set
summary(sf_crime)
```

*PdId* is a unique identifier (each incident gets its unique number) whereas *IncidntNum* incident number can be assigned to several incidents that happened during the same crime.
```{r include = FALSE}
# Find duplicates among "IncidntNum"(incident number)
duplicated(sf_crime$IncidntNum)
```

```{r warning=FALSE}
# Total number of duplicates among "IncidntNum"(incident number)
print(sum(duplicated(sf_crime$IncidntNum)))
```
33801 *IncidntNum* incident numbers have more than one crime category assigned to them. That means that 23% of all incidents happened in year 2016 involved more than one crime. 

```{r warning=FALSE}
# Find index of the duplicates among "IncidntNum" (incident ID)
dup <- sf_crime$IncidntNum[duplicated(sf_crime$IncidntNum)]

# Print 10 incident ID's with more than one crime
head(dup, 10) %>% knitr::kable("pipe")
```

The new processed data set contains 21 variables instead of original 13   
```{r warning=FALSE, include = FALSE}
# Processed sf_crime data
head(sf_crime, 10) 
```

```{r  warning=FALSE}
#   Description of variables in processed sf_crime
summary(sf_crime)
```

Number of unique crime incidents is 150,500, there are 39 listed crime categories. *Description* of crime includes 726 different descriptions. This variable was mot strictly defined and was up to the police officers to chose the description. 16130 different addresses are present in the data set. There are 10 districts (plus one incident without a district) of Police Department and 14 resolution of crime categories.
```{r warning=FALSE, echo = FALSE}
sf_crime %>% summarise(n_ids = n_distinct(PdId), 
                       n_crime_category = n_distinct(Category),
                       n_description    = n_distinct(Descript),
                       n_district       = n_distinct(PdDistrict),
                       n_resolution     = n_distinct(Resolution),
                       n_address        = n_distinct(Address), .groups = 'drop')  %>% ungroup() %>% knitr::kable("pipe")
```

# Visualization

##  District 
The occurrence of crime by 10 different districts shows that by far the most criminal district is Southern district followed by Northern and Mission. Park district shows the least crime occurrence with 8699 incidents for 2016.

```{r echo=FALSE, message=FALSE}
#   Number of crime occurrences for each district
sf_crime %>% group_by(PdDistrict) %>% 
  summarize(count=n())%>%
  ggplot(aes(x= reorder(PdDistrict, count), y = count))+
  geom_bar(stat='identity', alpha=.5, color = "black", fill = "red")+
  scale_fill_continuous(type = "viridis") +
  ggtitle("Occurrence of Crime by District") +
  coord_flip(y=c(0, 30000))+
  labs(x="District", y="Number of Crime Occurrences")+
  geom_text(aes(label= count), hjust=-0.1, size=3) +
  theme(plot.title = element_text(hjust = 0.5))
```

## Category 
The variable *Category* is a factor variable with 39 different levels. 

```{r}
# Crime Category
class(sf_crime$Category)
nlevels(sf_crime$Category)
print(unique(sf_crime$Category))
```

Following categories are the top 10 of total 39 categories. "LARCENY/THEFT" accounts for the most cases."MISSING PERSON" is the smallest category in the top ten of crime categories.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Top 10 crimes with the greatest number of occurrence
sf_crime %>% group_by(Category)%>%
  summarize(count = n(), .groups = 'drop')  %>% ungroup() %>% arrange(desc(count)) %>% 
  top_n(10, count) %>% knitr::kable()
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# TOP 10 most frequent Crimes by Category
sf_crime %>%group_by(Category) %>% 
  summarize(count=n(), .groups = 'drop')  %>% ungroup() %>% arrange(desc(count)) %>% top_n(10, count) %>%
  ggplot(aes(x = reorder(Category, -count), y = count)) +
  geom_bar(stat="identity", alpha=.5, color="black", fill= "blue") + theme_bw()+
  ggtitle("TOP 10 Crimes by Category") +
  labs(x = "Category", y = "Number of Occurrences")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Weekdays
The following pie-chart shows that crime occurrence over the week days was almost evenly distributed with Friday as a day most crime occurrence (only a small difference to other days). 

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Pie-Chart Crime Frequency on Weekdays
sf_crime %>% 
  group_by(DayOfWeek) %>% 
  summarize(count=n(), .groups = 'drop')  %>% ungroup() %>%
  mutate(percent = count /sum(count) *100.0) %>%
  ggplot( aes(x="", y = - percent, fill = DayOfWeek)) +
  geom_bar(width = 1, stat = "identity", color = "black") + coord_polar(theta = "y")+
  geom_text(aes(label = paste0(round(percent), "%", sep = "\n", DayOfWeek)), 
            position = position_stack(vjust = 0.5)) +
  labs(x = NULL, y = NULL, fill = NULL, 
       title = "Crime Occurance on Weekdays in Percentage") +
  guides(fill = guide_legend(reverse = TRUE)) +
  scale_fill_manual(values = c("orange", "red", "yellow", "blue",  "green", "grey", "purple")) +
  theme_classic() +
  theme(axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        plot.title = element_text(hjust = 0.5, color = "#666666"))
```

## Daytime
The dat time plot shows that most crime occurred around 18:00 (6pm) with N = 11280 cases.According to the data for 2016 the safest hour was 5am in the morning with N=1520 incidents.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Occurrence of crime during a day Time_Hour
sf_crime %>%group_by(Time_Hour) %>% 
  summarize(count=n(), count_min = min(count), count_max = max(count), .groups = 'drop')  %>% ungroup() %>%
  ggplot(aes(x = Time_Hour, y = count)) +
  geom_bar(stat="identity", alpha=.5, color="black", fill= "green") + theme_bw()+
  geom_vline(xintercept = 5, colour = "red") +
  geom_vline(xintercept = 18, colour = "red") +
  ggtitle("Hour of the Day") +
  labs(x = "Hour", y = "Number of Occurrences")+
  scale_x_continuous(breaks=seq(0, 23, by= 1))+
  geom_text(aes(label= count),vjust = -0.5, position = position_dodge(width = .60), hjust=0.5, size = 2, inherit.aes = TRUE) +
  theme(plot.title = element_text(hjust = 0.5))
```

Crime Category "LARCENY/THEFT" remains the most committed crime throughout the day. The number of "LARCENY/THEFT" increases in the evening hours (starting in the afternoon).

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Top 10 crime category over day time
sf_crime%>% group_by(Time_Hour, Category) %>% 
  summarise(count = n(), .groups = 'drop')  %>% ungroup() %>%
  filter(Category %in% c("LARCENY/THEFT", "OTHER OFFENSES", "NON-CRIMINAL", "ASSAULT",    
                         "VANDALISM", "VEHICLE THEFT", "WARRANTS", "BURGLARY", 
                         "SUSPICIOUS OCC", "MISSING PERSON")) %>%
  ggplot(aes(x = Time_Hour, y = count)) +
  geom_line(aes(color=Category)) +
  ggtitle("Hour of the Day") +
  scale_x_continuous(breaks=seq(0, 23, by= 1))+
  scale_fill_brewer(palette = "Paired")+ 
  theme(plot.title = element_text(hjust = 0.5))
```

## Months
The crime distribution among the months of 2016 also shows an even distribution with October with max number of crime incidents.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Number of Crime Occurrences on Weekdays
sf_crime  %>% group_by(Month) %>% 
 summarize(count=n(), .groups = 'drop')  %>% ungroup() %>% arrange(desc(count))  %>% 
  ggplot(aes(x = Month, y = count)) +
  geom_bar(stat="identity", alpha=.5, color="red", fill= "purple", position = 'dodge') + theme_bw()+
  scale_fill_continuous(type = "viridis") +
  ggtitle("Crimes on Weekdays") +
  labs(title = "Crime over Months",
           x = "Month", 
           y = "Number of Occurrences") +
  geom_text(aes(label= count),vjust = -0.5, position = position_dodge(width = .70), hjust=0.5, size = 2, inherit.aes = TRUE) + 
  scale_y_continuous(breaks=seq(0, 15000, by= 5000)) +
  theme_bw()  + 
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Resolution of Crime
An important variable in the crime statistics is a crime resolution variable. And as we can see most of the cases were not resolved. The second biggest category within the variable **Resolution** is "ARREST, BOOKED".

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Top 10 of most common crime resolutions
sf_crime %>%group_by(Resolution) %>% 
  summarize(count=n()) %>% arrange(desc(count)) %>% top_n(10, count) %>%
  ggplot(aes(x = reorder(Resolution, -count), y = count)) +
  geom_bar(stat="identity", alpha=.5, color="black", fill= "green") + theme_bw()+
  ggtitle("TOP 10 Crime Resolutions") +
  labs(x = "Resolution Category",
       y = "Number of Occurrences")+
  geom_text(aes(label= count),vjust = -0.5, position = position_dodge(width = .70),   hjust=0.5, size = 1, inherit.aes = TRUE) + 
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Among the top ten crimes category "ARREST, BOOKED" accounts most for "OTHER OFFENSES" and "ASSAULT", while category "NONE" resolutions is highest for "LARCENY/THEFT".

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Top 10 Crimes among 2 Resolution categories "NONE" and "BOOKED ARREST"
sf_crime %>%filter(Category %in% c("LARCENY/THEFT", "OTHER OFFENSES", "NON-CRIMINAL", "ASSAULT",    
                                   "VANDALISM", "VEHICLE THEFT", "WARRANTS", "BURGLARY", 
                                   "SUSPICIOUS OCC", "MISSING PERSON"), Resolution %in% c("NONE", "ARREST, BOOKED")) %>%
  group_by(Category) %>% 
  mutate(count=n()) %>% arrange(desc(count)) %>%
  ggplot(aes(x = reorder(Category, -count), y = count)) +
  geom_bar(stat="identity", alpha=.5, color = "blue", fill = "grey" ) + theme_bw()+
  scale_fill_continuous(type = "viridis") +
  ggtitle("TOP 10 Crimes by Category") +
  facet_wrap(~Resolution, ) +
  ggtitle("TOP 10 Crime Resolutions") +
  labs(x = "Resolution Category", y = "Number of Occurrences")+
  # geom_text(aes(label= count),vjust = -0.5, position = position_dodge(width = .60), hjust=0.5, size = 2, inherit.aes = TRUE) + 
  theme(plot.title = element_text(hjust = 0.5))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Description
The variable *Description* contains 726 different non predefined descriptions of crime and therefor can not be taken as a reliable source into the analysis. Following table shows to 10 of used descriptions to describe crime incidents.
```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Top 10 of the most frequent crime descriptions in San Francisco
sf_crime  %>% group_by(Descript) %>% 
             summarize(count=n(), .groups = 'drop')  %>% ungroup() %>% arrange(desc(count)) %>% 
             top_n(10, count) %>% knitr::kable()
```

## Address
Some addresses appeared more often in the data than others. There are 16130 different addresses in the data set. Following visualization shows top 20 most frequent addresses in the crime data for 2016.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Top 20 of the most frequent addresses in San Francisco
sf_crime  %>% group_by(Address) %>% 
  summarize(count=n(), .groups = 'drop')  %>% ungroup() %>% arrange(desc(count)) %>% 
  top_n(20, count) %>% knitr::kable()
```

Number one address in the top ten that occurred 3561 times in 2016 is "800 Block of BRYANT ST" which is the address of a county jail.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Most common Address
sf_crime %>% group_by(Address) %>%
  summarize(count = n())  %>% arrange(desc(count)) %>% top_n(20) %>%
  ggplot(aes(reorder(Address, count), count, fill = count)) +
  geom_bar(stat = "identity") + coord_flip() + scale_fill_distiller(palette = "PuBuGn") +
  ggtitle("") + xlab("Address") +
  ggtitle("Address for Number of Crime Occurrences") +
  theme_classic()
```

# Method
The goal of the analysis id to predict the category of crime by using predictors considered in the Visualization part. First take a look at the correlation between the variables.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Correlation between numerical variables
cor.data <- cor(sf_crime[, c("Resolution_dummy","Category_Violent_dummy", 
                             "Count_Address_Occur", "Time_Hour","Date_Day",
                             "Date_Month", "X", "Y")]) %>% as.matrix
corrplot(cor.data, order = "hclust", addrect = 2, type = "lower")


```

In order to predict categorical variable *Logistic Regression*[^1] will be applied. As an outcome of the prediction *Category_Violent_dummy* with assigned values of 0 (non-violent) and 1 (violent) will be used. *Logistic Regression* is a subset of the *Generalized linear Models* and converts probability to log odds. (More details are given in 31.1 Chapter of https://rafalab.github.io/dsbook/).

Accuracy was used to judge the performance of the prediction of a categorical variable. For the evaluation of the algorithm a **validation** data set was generated. The **sf_crime** data set was partitioned into **validation** and **sf_crime_p**.  The **validation** data set was only used in the final step to test the final algorithm and contained only 20% of **sf_crime** data. The final model with the highest accuracy was chosen to be applied to the **sf_crime_p** data to calculate the parameters of the model. For the final step, this model was evaluated by calculating the accuracy of the **validation** set. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Code to generate the validation set
set.seed(342)
test_index <- createDataPartition(y = sf_crime$Category_Violent_dummy , times = 1, p = 0.2, list = FALSE)
sf_crime_p <- sf_crime[-test_index,]
temp <- sf_crime[test_index,]

#   Make sure "IncidntNum" in validation set are also in sf_crime_p set
validation <- temp %>% 
          semi_join(sf_crime_p, by = "IncidntNum")
  
# Add rows removed from validation set back into sf_crime_p set
removed <- anti_join(temp, validation)
sf_crime_p <- rbind(sf_crime_p, removed)

#  Remove redundant columns
rm(test_index, temp, removed)  
```  

In order to train and test the algorithm, **sf_crime_p** was divided into train and test set, where test_set contained only 20% of the **sf_crime_p** data.

```{r echo=FALSE, message=FALSE, warning=FALSE}

#   Generate train and test sets, 20% of sf_crime_p data   
set.seed(110)
test_index <- createDataPartition(y = sf_crime_p$Category_Violent_dummy, times = 1, p = 0.2, list = FALSE)
test_set <- sf_crime_p[test_index, ]
train_set <- sf_crime_p[-test_index, ]  

#   To make sure we donâ€™t include incident Ids in the test set that do not appear in the training set, 
#   remove these entries using the semi_join function:
test_set <- test_set %>% 
        semi_join(train_set, by = "IncidntNum")
```

## Logistic regression with one predictor   
Only numerical variables were use as predictors in the *Logistic Regression* model. First only one predictor *Resolution_dummy*, a dummy variable with values of 1 for resolved crime and value of 0 for non-resolved crime. 
$glm()$ function with specified $family = "binomial"$ is used to fit the *Logistic Regression* model.

```{r  warning=FALSE}
# Fit Logistic regression with one predictor
fit_glm <- glm(Category_Violent_dummy ~ Resolution_dummy,
               data=train_set, family = "binomial")

# Predict "Category_Violent_dummy"
p_hat_glm <- predict(fit_glm, test_set, type = "response")
```

The decision rule follows, predict violent Category if prediction $p_hat_glm > 0.15$, otherwise $0$.
```{r  warning=FALSE}
# Set values to 1 (violent) if > 0.15 and otherwise 0 (non-violent)
y_hat_glm <- ifelse(p_hat_glm > 0.15, 1, 0) 
```


```{r  warning=FALSE}
#   Set factors to the same factor level
y_hat_glm<-y_hat_glm %>%factor()
test_set$Category_Violent_dummy<-test_set$Category_Violent_dummy %>%factor()
```

```{r warning= FALSE, include = FALSE}
#   Check the factor levels
nlevels(y_hat_glm )
nlevels(test_set$Category_Violent_dummy)
```

Overall accuracy is low and is below the guessing rate. The proportion of violent crime is much lower than a non-violent crime. To check *sensitivity* and *specificity*.

```{r  warning=FALSE}
#  Show byClass to check sensitivity, specificity 
confusionMatrix(y_hat_glm, test_set$Category_Violent_dummy)$byClass %>% knitr::kable()
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
# Show accuracy of the model
accuracy_1<- confusionMatrix(y_hat_glm, test_set$Category_Violent_dummy)$overall["Accuracy"]
accuracy_results <- data_frame(Model = "Logistic regression with one predictor", Accuracy = accuracy_1)
```

```{r}
# Print results
accuracy_results %>% knitr::kable()
```


The result shows the Accuracy is below the guessing rate, next model will include following predictors:
- *Resolution_dummy*- shows if crime was resolved
- *Time_Hour* - time of the day 
- *Date_Day* - day ot he month
- *Date_Month* - which month of the year
- *Count_Address_Occur* - frequency how often crime took place
- *X* and *Y* -Location of the crime


## Logistic regression with more than one predictor 

```{r  warning=FALSE}
fit_glm_mp <- glm(Category_Violent_dummy ~ Resolution_dummy + Time_Hour + Date_Day + 
                  Date_Month + Count_Address_Occur + X + Y,
                  family = "binomial", data = train_set)

#   Predict "Category_Violent_dummy"
p_hat_glm_mp <- predict(fit_glm_mp, test_set, type = "response")

#  Set values to 1 (violent) if > 0.15 and otherwise 0 (non-violent)
y_hat_glm_mp <- ifelse(p_hat_glm_mp > 0.15, 1, 0) 

#   Set factors to the same factor level
y_hat_glm_mp <- y_hat_glm_mp %>% factor()
test_set$Category_Violent_dummy <- test_set$Category_Violent_dummy %>% factor()
```

```{r warning= FALSE, include = FALSE}
# Check the factor levels
nlevels(y_hat_glm_mp )
nlevels(test_set$Category_Violent_dummy)
```

```{r  warning=FALSE}
#  Show byClass to check sensitivity, specificity 
confusionMatrix(y_hat_glm_mp, test_set$Category_Violent_dummy)$byClass %>% knitr::kable()
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Show accuracy of the model
accuracy_2<- confusionMatrix(y_hat_glm_mp, test_set$Category_Violent_dummy)$overall["Accuracy"]
accuracy_results <- bind_rows(accuracy_results,
                              data_frame(Model = "Logistic regression with more than one predictor",
                                         Accuracy = accuracy_2))

```


```{r  warning=FALSE}
# Print results
accuracy_results %>% knitr::kable()
```

Since the result has improved and is above the guessing rate, this model will be applied on the **validation** and **sf_crime_p** data.

## Final Logistic regression with more than one predictor on **validation** and **sf_crime_p**

```{r  warning=FALSE}
fit_glm_mp_v <- glm(Category_Violent_dummy ~ Resolution_dummy + Time_Hour + Date_Day + 
                    Date_Month + Count_Address_Occur + X + Y,
                    family = "binomial", data=sf_crime_p)

# Predict "Category_Violent_dummy"
p_hat_glm_mp_v <- predict(fit_glm_mp_v, validation, type = "response")

# Set values to 1 (violent) if > 0.15 and otherwise 0 (non-violent)
y_hat_glm_mp_v <- ifelse(p_hat_glm_mp_v > 0.15, 1, 0) 

# Set factors to the same factor level
y_hat_glm_mp_v <- y_hat_glm_mp_v %>% factor()
validation$Category_Violent_dummy <- validation$Category_Violent_dummy %>% factor()
```


```{r warning= FALSE, include = FALSE}
# Check the factor levels
nlevels(y_hat_glm_mp_v )
nlevels(validation$Category_Violent_dummy)
```

```{r  warning=FALSE}
# Show byClass to check the sensitivity, specificity 
confusionMatrix(y_hat_glm_mp_v, validation$Category_Violent_dummy)$byClass %>% knitr::kable()
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#   Show accuracy of the model
accuracy_final <- confusionMatrix(y_hat_glm_mp_v, validation$Category_Violent_dummy)$overall["Accuracy"]
accuracy_results <- bind_rows(accuracy_results,
                              data_frame(Model = "Final Logistic regression with more than one predictor",
                                         Accuracy = accuracy_final))
```



```{r  warning=FALSE}
#   Print results
accuracy_results %>% knitr::kable()
```


## Principal component Analyses (PCA)  
Another method will be applied to improve the results of the prediction. PCA is is one of ML methods to reduce high-dimensionality of the data set in case of many predictors. PCA works best with numerical data all categorical variables are excluded from the data with predictors.

```{r message=FALSE, warning=FALSE}
train_set_pca <- train_set[ , c("Resolution_dummy",
                                "Count_Address_Occur", "Time_Hour",
                                "Date_Day", "Date_Month", "X", "Y") ]

test_set_pca <- test_set[ , c("Resolution_dummy", 
                              "Count_Address_Occur", "Time_Hour",
                              "Date_Day", "Date_Month", "X", "Y") ]
```

This data is passed to the $prcomp()$ function assigning the output to **sf_crime.pca**.  $prcomp()$ provides linear orthogonal transformation of the passed data.Argument $center = TRUE$ means the columns are centered. Argument $scale = TRUE$ only makes only sense if variables are measured on the same scale, which is not the case. The center component corresponds to the means of the variables.

```{r message=FALSE, warning=FALSE}
# PCA method
sf_crime.pca <- prcomp(train_set_pca, center = TRUE)

# Mean value
sf_crime.pca$center
```

PCA returns 3 components: $\$x$ the principal components, $\$rotation$ to transform the matrix and $\$sdev$ standard deviation.

```{r warning = FALSE}
# PC's components
head(sf_crime.pca$x)

# Standard Deviation of the components
sf_crime.pca$sdev

# Rotation parameter
head(sf_crime.pca$rotation)
```

7 principal components were obtained. Each PC explains a percentage of the total variation in the data. PC1, PC2 and PC3 explain almost 100% of the total variance.

```{r message=FALSE, warning=FALSE}
# Summary of the output
summary(sf_crime.pca)
summary(sf_crime.pca)$importance

# Calculate the Variance
sf_crime.pca.var <- sf_crime.pca$sdev^2
sf_crime.pca$sdev^2
```

The percentage of variation explained by PC's.
```{r message=FALSE, warning=FALSE}
# Percentage of Variation
sf_crime.pca.var.per <- sf_crime.pca.var/sum(sf_crime.pca.var) * 100
sf_crime.pca.var.per
```

Here we can see what variables have the largest effect

```{r message=FALSE, warning=FALSE}
sf_crime.pca.rotation <- sf_crime.pca$rotation[ , 1]
sort(abs(sf_crime.pca.rotation) , decreasing = TRUE) %>% knitr::kable()
```

## Visualization of the Variation

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(sf_crime.pca, type="lines")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Visualization of the explained variation
fviz_eig(sf_crime.pca, addlabels = TRUE)
```

Plotting first two components of PCA. The plot shows how close are the observations to each other if they build clusters.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#  Plot PC1 and PC2 to have more information
data.frame(PC1 = sf_crime.pca$x[,1], PC2 = sf_crime.pca$x[,2], label=factor(train_set$Category_Violent_dummy)) %>%
  sample_n(1000) %>%
  ggplot(aes(PC1, PC2, fill=label))+
  geom_point(cex=3, pch=21) 
```

In next step transformation will be applied to train and test data. Knn method will be fit on the reduced data set with relevant predictors.

```{r message=FALSE, warning=FALSE}
# Preparation for model fitting: calculate means of Columns 
col_means<- colMeans(test_set_pca)
#  Set x_train equal to PC's
x_train <- sf_crime.pca$x[ ,1:3]
y <- factor(train_set$Category_Violent_dummy)
#  Fit knn model
fit <- knn3(x_train, y)
#  Transform the test_set
x_test <- as.matrix(sweep(test_set_pca, 2, col_means)) %*% sf_crime.pca$rotation
x_test <- x_test[ ,1:3] 
#  Predict Category 
y_hat <- predict(fit, x_test, type = "class")
# Print Accuracy of the model
confusionMatrix(y_hat,
                factor(test_set$Category_Violent_dummy))$overall["Accuracy"] %>%
                 knitr::kable("pipe")
```

## Final PCA on  **validation** and **sf_crime_p**
Since the Accuracy improved by using the PCA, the same procedure will be applied to the **validation** and **sf_crime_p**.
```{r message=FALSE, warning=FALSE}
#  Keep only numerical variables in sf_crime_p_pca data set
sf_crime_p_pca <- sf_crime_p[ , c( "Resolution_dummy",
                                   "Count_Address_Occur", "Time_Hour","Date_Day",
                                   "Date_Month", "X", "Y") ]

#  Keep only numerical variables in validation_pca data set
validation_pca <- validation[ , c( "Resolution_dummy",
                                   "Count_Address_Occur", "Time_Hour","Date_Day",
                                   "Date_Month", "X", "Y") ]

#  Perform PCA 
sf_crime.pca <- prcomp(sf_crime_p_pca, center = TRUE)

# Preparation for model fitting: calculate means of columns 
col_means<- colMeans(validation_pca)

#  Set x_train equal to PC's
x_train <- sf_crime.pca$x[ ,1:3]

y <- factor(sf_crime_p$Category_Violent_dummy)

#  Fit knn with k = 5 model
fit <- knn3(x_train, y)

#  Transform the test_set
x_test <- as.matrix(sweep(validation_pca, 2, col_means)) %*% sf_crime.pca$rotation
x_test <- x_test[ ,1:3] 

#  Predict Category 
y_hat <- predict(fit, x_test, type = "class")

# Print Accuracy of the model on Validation data
confusionMatrix(y_hat,
                factor(validation$Category_Violent_dummy))$overall["Accuracy"] %>%
                knitr::kable("pipe")
```

# Results and Disscussion
Due to the computational power the a small data set was chosen which led to limitations in the chose of models. The goal of the analysis was to predict the crime category. There are 39 different crime categories  listed. Analysis applied to predict variable *Category* yielded a very low Accuracy. Some of the 39 Categories don't include enough observations or information to build a reliable prediction. Therefore the variable *Category* was transform into a dummy variable *Category_Violent_dummy* was assigned value 1 when "ASSAULT", "ROBBERY", "SEX OFFENSES, FORCIBLE", "KIDNAPPING" and value 0 otherwise. All performed methods first were applied on train and test and additionally on the **validation** and **sf_crime_p** data.
First *Logistic Regression* with one predictor was applied with $Accuracy = 0.427$ with is below the guessing rate. By extending the number of predictors by 7 the the Accuracy increased to 63%. This model was fit to the **validation** and **sf_crime_p** data and showed $Accuracy = 0.626$.

In order to improve the results and to reduce the number of predictors *Principal Component Analysis* (PCA) was used. First the method was applied to the train data with 7 predictors such as: *Resolution_dummy*, *Count_Address_Occur*, *Time_Hour*,  *Date_Day*, *Date_Month*, *X* and *Y* for location. PCA showed that *Count_Address_Occur*, *Date_Day* and *Date_Month* have the largest effect and that first 3 PC's explain almost 100% of the variation in the data. Therefore only first 3 PC's were taken into account to predict *Category_Violent_dummy*. The obtained result $Accuracy = 0.798$. As conclusion results obtained from PCA yielded higher $Accuracy$ with less predictors than results obtained from *Logistic Regression*. 
The difficulty of having the memory capacity limitations led to the reduction of the analysis techniques. 



[^1]: https://rafalab.github.io/dsbook/